{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow! There is no error here!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "corpus = open(\"corpus.txt\", \"r\", encoding=\"UTF8\")\n",
    "#scaning\n",
    "sentence = corpus.readline()\n",
    "line=1\n",
    "def isKorean(x):\n",
    "    p = re.compile(\"[\\u3131-\\u3163\\uac00-\\ud7a3]+\")\n",
    "    return p.search(x)\n",
    "maxerror = 10\n",
    "error = 0\n",
    "while sentence:\n",
    "    ko = isKorean(sentence[:-1])\n",
    "    if (line % 2 == 1  and ko) or (line % 2 == 0 and ko == None):\n",
    "        print(\"error occur: line\", line, \"\\nsentence : \", sentence,end=\"\")\n",
    "        if line % 2 == 1:\n",
    "            print(\"Reason: odd line should consist of English letter, not Korean.\\n\")\n",
    "        else :\n",
    "            print(\"Reason: even line should have at least one Korean letter.\\n\")\n",
    "        error += 1\n",
    "    if error >= maxerror :\n",
    "        print(\"\\nand so on...\")\n",
    "        break\n",
    "    sentence = corpus.readline()\n",
    "    line += 1\n",
    "if error == 0:\n",
    "    print(\"Wow! There is no error here!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(\"corpus.txt\",\"r\", encoding=\"UTF8\")\n",
    "eng = open(\"eng.txt\",\"w\",encoding=\"UTF8\")\n",
    "kor = open(\"kor.txt\",\"w\",encoding=\"UTF8\")\n",
    "sentence = corpus.readline()\n",
    "line = 1\n",
    "while sentence:\n",
    "    if line % 2 == 1:\n",
    "        eng.write(sentence)\n",
    "    else :\n",
    "        kor.write(sentence)\n",
    "    sentence = corpus.readline()\n",
    "    line += 1\n",
    "corpus.close()\n",
    "eng.close()\n",
    "kor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['크', 'ㄴ', '거리', '는', '연중', '내내', '번잡', '하', '다']\n",
      "['월드컵', '은', '온', '나라', '에', '사람', '들', '을', '즐겁', '게', '하', '였', '다', '.']\n",
      "['태양', '은', '지구의', '지름', '의', '109', '배', '이', '다']\n",
      "['(', '고어', ')', '여론', '.']\n",
      "['그', '시골', '소년', '은', '대도시', '에서', '어색', '하', '어', '보이', '었', '다', '.']\n",
      "['백문', '이', '불이', '어', '일견', '이', '란', '말', '도', '있', '잖아', '.']\n",
      "['그', '실험', '의', '통제', '집단', '.']\n",
      "['일', '년', '중', '에', '가장', '짧', '은', '날', '.']\n",
      "['남자', '가', '기계', '를', '수리', '점', '으로', '가져가', '고', '있', '다', '.']\n",
      "['나', '는', '주인', '의', '초대', '로', '파티', '에', '가', '었', '었', '다', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "tag = Kkma()\n",
    "kor = open(\"kor.txt\", 'r', encoding=\"UTF8\")\n",
    "for i in range(10):\n",
    "    sentence = kor.readline()\n",
    "    print(tag.morphs(sentence))\n",
    "kor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-490ce7d7b9e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mcount\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkorcor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mmost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mkor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\POS KoNLPy\\lib\\site-packages\\konlpy\\tag\\_kkma.py\u001b[0m in \u001b[0;36mmorphs\u001b[1;34m(self, phrase)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;34m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\POS KoNLPy\\lib\\site-packages\\konlpy\\tag\\_kkma.py\u001b[0m in \u001b[0;36mpos\u001b[1;34m(self, phrase, flatten, join)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \"\"\"\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjki\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmorphAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mmorphemes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\POS KoNLPy\\lib\\site-packages\\jpype\\_jclass.py\u001b[0m in \u001b[0;36m_getClassFor\u001b[1;34m(javaClass)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_getClassFor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjavaClass\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjavaClass\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_CLASSES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from konlpy.tag import Kkma\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "tag = Kkma()\n",
    "kor = open(\"kor.txt\", 'r', encoding=\"UTF8\")\n",
    "\n",
    "korcor = kor.read().split(\"\\n\")\n",
    "mask = np.random.choice(692109, 180000, replace=False)\n",
    "count = Counter()\n",
    "for i in mask :\n",
    "    count.update(tag.morphs(korcor[i]))\n",
    "most = count.most_common(100)\n",
    "kor.close()\n",
    "for word, time in most:\n",
    "    print(word,\":\",time)\n",
    "del tag\n",
    "del np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(count))\n",
    "words = [(\"UNK\",0), (\"<s>\",1),(\"</s>\", 2)]\n",
    "most = count.most_common(14997)\n",
    "words.extend(most)\n",
    "del most\n",
    "del count\n",
    "dictionary = {}\n",
    "for i in range(len(words)):\n",
    "    word, _ = words[i]\n",
    "    dictionary[word] = i\n",
    "del words\n",
    "reverse_dictionary = dict(zip(dictionary.values(),dictionary.keys()))\n",
    "with open(\"kor_dictionary.bin\",\"wb\") as f:\n",
    "    pickle.dump(dictionary, f)\n",
    "with open(\"kor_reverse_dictionary.bin\",\"wb\") as f:\n",
    "    pickle.dump(reverse_dictionary, f)\n",
    "del reverse_dictionary\n",
    "with open(\"kor.txt\",\"r\",encoding=\"UTF8\") as f:\n",
    "    indexed = open(\"kor_indexed.bin\", \"wb\")\n",
    "    parser = Kkma()\n",
    "    sentence = f.readline()\n",
    "    ids = []\n",
    "    while sentence:\n",
    "        parsing = parser.morphs(sentence)\n",
    "        for i in range(len(parsing)):\n",
    "            if parsing[i] in dictionary:\n",
    "                parsing[i] = dictionary[parsing[i]]\n",
    "            else :\n",
    "                parsing[i] = 0\n",
    "            if i % 10000 == 0 :\n",
    "                print(i,\"번째\")\n",
    "        ids.append(parsing)\n",
    "        sentence = f.readline()\n",
    "    pickle.dump(ids, indexed)\n",
    "    indexed.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "tag = nltk()\n",
    "eng = open(\"eng.txt\", 'r', encoding=\"UTF8\")\n",
    "\n",
    "engcor = eng.read().split(\"\\n\")\n",
    "mask = np.random.choice(692109, 692109, replace=False)\n",
    "count = Counter()\n",
    "for i in mask :\n",
    "    count.update(tag.word_tokenize(eng.cor[i]))\n",
    "most = count.most_common(100)\n",
    "eng.close()\n",
    "for word, time in most:\n",
    "    print(word,\":\",time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(count))\n",
    "words = [(\"UNK\",0), (\"<s>\",1),(\"</s>\", 2)]\n",
    "most = count.most_common(60000)\n",
    "words.extend(most)\n",
    "dictionary = {}\n",
    "for i in range(len(words)):\n",
    "    word, _ = words[i]\n",
    "    dictionary[word] = i\n",
    "reverse_dictionary = dict(zip(dictionary.values(),dictionary.keys()))\n",
    "with open(\"eng.txt\",\"r\",encoding=\"UTF8\") as f:\n",
    "    indexed = open(\"eng_indexed.bin\", \"wb\")\n",
    "    parser = nltk()\n",
    "    sentence = f.readline()\n",
    "    ids = []\n",
    "    while sentence:\n",
    "        parsing = parser.word_tokenize(sentence)\n",
    "        for i in range(len(parsing)):\n",
    "            if parsing[i] in dictionary:\n",
    "                parsing[i] = dictionary[parsing[i]]\n",
    "            else :\n",
    "                parsing[i] = dictionary[\"UNK\"]\n",
    "        ids.append(parsing)\n",
    "        sentence = f.readline()\n",
    "    pickle.dump(ids, indexed)\n",
    "    indexed.close()\n",
    "with open(\"eng_dictionary.bin\",\"wb\") as f:\n",
    "    pickle.dump(dictionary, f)\n",
    "with open(\"eng_reverse_dictionary.bin\",\"wb\") as f:\n",
    "    pickle.dump(reverse_dictionary, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
