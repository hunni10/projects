{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK\n",
      "<s>\n",
      "</s>\n",
      "the\n",
      "30000\n",
      "의\n",
      "30000\n",
      "30000\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "#토큰을 단어로 바꾸는 영어토큰 역-딕셔너리\n",
    "#예: <4> -> \"the\"\n",
    "#크기는 3만\n",
    "with open(\"eng_reverse_dictionary.bin\",\"rb\") as file :\n",
    "    eng_rdictionary = pickle.load(file)\n",
    "    \n",
    "#역딕셔너리의 0번째는 알수 없는 토큰 UNK\n",
    "print(eng_rdictionary[0])\n",
    "#역딕셔너리의 1번째는 시계열 시작 토큰 <s>\n",
    "print(eng_rdictionary[1])\n",
    "#역딕셔너리의 2번째는 시계열 끝 토큰 </s>\n",
    "print(eng_rdictionary[2])\n",
    "\n",
    "\n",
    "print(eng_rdictionary[4])\n",
    "print(len(eng_rdictionary.keys()))\n",
    "\n",
    "#토큰을 단어로 바꾸는 한글토큰 역-딕셔너리\n",
    "#예: <10> -> \"의\"\n",
    "with open(\"kor_reverse_dictionary.bin\",\"rb\") as file :\n",
    "    kor_rdictionary = pickle.load(file)\n",
    "\n",
    "print(kor_rdictionary[10])\n",
    "print(len(kor_rdictionary.keys()))\n",
    "\n",
    "#영어 단어 토큰을 벡터로 바꾸는 임베딩 정보\n",
    "with open(\"eng_embedding.bin\",\"rb\") as file :\n",
    "    eng_embedding = pickle.load(file)\n",
    "print(len(eng_embedding))\n",
    "\n",
    "#한글 단어 토큰을 벡터로 바꾸는 임베딩 정보\n",
    "with open(\"kor_embedding.bin\",\"rb\") as file :\n",
    "    kor_embedding = pickle.load(file)\n",
    "print(len(kor_embedding))\n",
    "\n",
    "\n",
    "#토큰 번호로 이루어진 영문 데이터 불러오기\n",
    "with open(\"eng_indexed.bin\", \"rb\") as file:\n",
    "    #maxlen = [-1]\n",
    "    eng_indexed = []\n",
    "    #tmp_i = -1\n",
    "    for i in range(692109):\n",
    "        tmp_s = pickle.load(file)\n",
    "        eng_indexed.append(tmp_s)\n",
    "        #if len(tmp_s) > maxlen[-1] : \n",
    "        #    maxlen.append(len(tmp_s))\n",
    "        #    tmp_i = i\n",
    "#print(maxlen[-1], tmp_i, eng_indexed[tmp_i])\n",
    "\n",
    "\n",
    "#토큰 번호로 이루어진 국문 데이터 불러오기\n",
    "with open(\"kor_indexed.bin\", \"rb\") as file:\n",
    "    #maxlen = [-1]\n",
    "    kor_indexed = []\n",
    "    #tmp_i = -1\n",
    "    for i in range(692109):\n",
    "        tmp_s = pickle.load(file)\n",
    "        kor_indexed.append(tmp_s)\n",
    "        #if len(tmp_s) > maxlen[-1] : \n",
    "        #    maxlen.append(len(tmp_s))\n",
    "        #    tmp_i = i\n",
    "#print(maxlen[-1], tmp_i, kor_indexed[tmp_i])\n",
    "\n",
    "del tmp_s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "eng_rdict_size = len(eng_rdictionary)\n",
    "kor_rdict_size = len(kor_rdictionary)\n",
    "embedding_size = len(eng_embedding[0])\n",
    "en_length = 60\n",
    "kr_length = 60\n",
    "sentences_num = len(eng_indexed)\n",
    "num_epochs = 5\n",
    "batch_size = 96\n",
    "num_units = 256\n",
    "learning_rate = 0.0001\n",
    "training = True\n",
    "\n",
    "if not training:\n",
    "    import nltk\n",
    "    #nltk.download()\n",
    "    batch_size = 1\n",
    "    with open(\"eng_dictionary.bin\",\"rb\") as file :\n",
    "        eng_dictionary = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_eng(tokens):\n",
    "  sentences = []\n",
    "  for i in range(tokens.shape[0]):\n",
    "    s = \"\"\n",
    "    for j in tokens[i]:\n",
    "      if j == 2:\n",
    "        break;\n",
    "      s += eng_rdictionary[j]+\" \"\n",
    "    sentences.append(s)\n",
    "  return sentences\n",
    "\n",
    "def token_to_kor(tokens):\n",
    "  sentences = []\n",
    "  for i in range(tokens.shape[0]):\n",
    "    s = \"\"\n",
    "    for j in tokens[i]:\n",
    "      if j == 2:\n",
    "        break;\n",
    "      s += kor_rdictionary[j]+\" \"\n",
    "    sentences.append(s)\n",
    "  return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 93, 841, 15370, 4, 50, 479, 4, 156, 3]\n",
      "4 --> the\n",
      "93 --> world\n",
      "841 --> cup\n",
      "15370 --> amused\n",
      "4 --> the\n",
      "50 --> people\n",
      "479 --> across\n",
      "4 --> the\n",
      "156 --> country\n",
      "3 --> .\n",
      "[3116, 11, 1332, 329, 13, 44, 18, 9, 1784, 32, 4, 166, 7, 3]\n",
      "3116 --> 월드컵\n",
      "11 --> 은\n",
      "1332 --> 온\n",
      "329 --> 나라\n",
      "13 --> 에\n",
      "44 --> 사람\n",
      "18 --> 들\n",
      "9 --> 을\n",
      "1784 --> 즐겁\n",
      "32 --> 게\n",
      "4 --> 하\n",
      "166 --> 였\n",
      "7 --> 다\n",
      "3 --> .\n"
     ]
    }
   ],
   "source": [
    "print(eng_indexed[1])\n",
    "for token in eng_indexed[1]:\n",
    "  print(token, \"-->\", eng_rdictionary[token])\n",
    "\n",
    "print(kor_indexed[1])\n",
    "for token in kor_indexed[1]:\n",
    "  print(token, \"-->\", kor_rdictionary[token])\n",
    "\n",
    "#inp, targ, tmp_eng_size, tmp_kor_size = batch_generator.make_batch(1, en_length, kr_length)\n",
    "#enc_emb = eng_embedding\n",
    "#\n",
    "#enc_inp = tf.nn.embedding_lookup(enc_emb, inp)\n",
    "#print(enc_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0   467   608    11  1677    49     4   213    99  5023   100     7\n",
      "      4   102     3     2     2     2     2     2     2     2     2     2\n",
      "      2     2     2     2     2     2     2     2     2     2     2     2\n",
      "      2     2     2     2     2     2     2     2     2     2     2     2\n",
      "      2     2     2     2     2     2     2     2     2     2     2     2]\n",
      " [    4    93   841 15370     4    50   479     4   156     3     2     2\n",
      "      2     2     2     2     2     2     2     2     2     2     2     2\n",
      "      2     2     2     2     2     2     2     2     2     2     2     2\n",
      "      2     2     2     2     2     2     2     2     2     2     2     2\n",
      "      2     2     2     2     2     2     2     2     2     2     2     2]\n",
      " [    4   579    11     0   213     4  4644     7     4   443     3     2\n",
      "      2     2     2     2     2     2     2     2     2     2     2     2\n",
      "      2     2     2     2     2     2     2     2     2     2     2     2\n",
      "      2     2     2     2     2     2     2     2     2     2     2     2\n",
      "      2     2     2     2     2     2     2     2     2     2     2     2]\n",
      " [    4  2568   711     7     4    50     2     2     2     2     2     2\n",
      "      2     2     2     2     2     2     2     2     2     2     2     2\n",
      "      2     2     2     2     2     2     2     2     2     2     2     2\n",
      "      2     2     2     2     2     2     2     2     2     2     2     2\n",
      "      2     2     2     2     2     2     2     2     2     2     2     2]\n",
      " [    4   394    32     4   156   616     0     9     4   250   164     3\n",
      "      2     2     2     2     2     2     2     2     2     2     2     2\n",
      "      2     2     2     2     2     2     2     2     2     2     2     2\n",
      "      2     2     2     2     2     2     2     2     2     2     2     2\n",
      "      2     2     2     2     2     2     2     2     2     2     2     2]]\n",
      "[15 10 11  6 12]\n",
      "[[   99     8   473     5 10670  1974 18318     4     7     2     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1]\n",
      " [ 3116    11  1332   329    13    44    18     9  1784    32     4   166\n",
      "      7     3     2     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1]\n",
      " [ 1216    11  4612 10373    10 22757   370     6     7     2     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1]\n",
      " [   42  7471    43  3365     3     2     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1]\n",
      " [   19  3255   865    11  5385    25 12014     4    17   125    12     7\n",
      "      3     2     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1]]\n",
      "[10 15 10  6 14]\n",
      "[0, 467, 608, 11, 1677, 49, 4, 213, 99, 5023, 100, 7, 4, 102, 3]\n",
      "[4, 93, 841, 15370, 4, 50, 479, 4, 156, 3]\n",
      "[4, 579, 11, 0, 213, 4, 4644, 7, 4, 443, 3]\n",
      "[4, 2568, 711, 7, 4, 50]\n",
      "[4, 394, 32, 4, 156, 616, 0, 9, 4, 250, 164, 3]\n",
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False]\n",
      " [ True  True  True  True  True  True  True  True  True  True False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False]\n",
      " [ True  True  True  True  True  True False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False]]\n",
      "[[ True  True  True  True  True  True  True  True  True  True False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False]\n",
      " [ True  True  True  True  True  True  True  True  True  True False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False]\n",
      " [ True  True  True  True  True  True False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False]]\n"
     ]
    }
   ],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, en, kr, totalnum = 692109, shuffle = True):\n",
    "        self.totalnum = totalnum\n",
    "        self.en = en\n",
    "        self.kr = kr\n",
    "        self.cursor = 0\n",
    "        self.mask = np.arange(self.totalnum)\n",
    "        self.shuffle = shuffle\n",
    "        if self.shuffle :\n",
    "            np.random.shuffle(self.mask)\n",
    "    def make_batch(self, batch_size, en_length = 60, kr_length = 40):\n",
    "        tmp_eng = np.ndarray((batch_size, en_length), dtype = np.int32)\n",
    "        tmp_eng_size  = np.ndarray((batch_size), dtype = np.int32) \n",
    "        tmp_kor = np.ndarray((batch_size, kr_length), dtype = np.int32)\n",
    "        tmp_kor_size  = np.ndarray((batch_size), dtype = np.int32)\n",
    "        tmp_eng_mask = np.zeros([batch_size, en_length], dtype=bool)\n",
    "        tmp_kor_mask = np.zeros([batch_size, kr_length], dtype=bool)\n",
    "        for i in range(batch_size):\n",
    "            tmps = self.en[self.mask[self.cursor]]\n",
    "            if len(tmps) >= en_length :\n",
    "                tmp_eng_size[i] = en_length\n",
    "                tmps = tmps[:en_length]\n",
    "            if len(tmps) < en_length :\n",
    "                tmp_eng_size[i] = len(tmps)\n",
    "                tmps = tmps + ([2] * (en_length-len(tmps)))\n",
    "            tmp_eng[i] = np.array(tmps)\n",
    "            tmps = self.kr[self.mask[self.cursor]]\n",
    "            if len(tmps) >= kr_length :\n",
    "                tmp_kor_size[i] = kr_length\n",
    "                tmps = tmps[:kr_length-1] + [2]\n",
    "            if len(tmps) < kr_length :\n",
    "                tmp_kor_size[i] = len(tmps) + 1\n",
    "                tmps = tmps + [2] + ([1] * (kr_length-len(tmps)-1))\n",
    "            tmp_kor[i] = np.array(tmps)\n",
    "            self.cursor += 1\n",
    "            if self.cursor >= self.totalnum :\n",
    "                self.cursor = 0\n",
    "                if self.shuffle :\n",
    "                    np.random.shuffle(self.mask)\n",
    "            tmp_eng_mask[i, :tmp_eng_size[i]] = 1\n",
    "            tmp_kor_mask[i, :tmp_kor_size[i]] = 1\n",
    "        tmp_eng = np.array(tmp_eng, dtype=np.int32)\n",
    "        tmp_kor = np.array(tmp_kor, dtype=np.int32)\n",
    "        \n",
    "\n",
    "        \n",
    "        return tmp_eng, tmp_kor, tmp_eng_size, tmp_kor_size, tmp_eng_mask, tmp_kor_mask\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "kor_indexed = []\n",
    "i = 0\n",
    "for _ in range(692109):\n",
    "    i = 8 + (i * 5 % 87)\n",
    "    kor_indexed.append(list(range(i)))\n",
    "#print(kor_indexed[0:100])\n",
    "\"\"\"\n",
    "\n",
    "batch_generator = BatchGenerator(eng_indexed, kor_indexed, totalnum = sentences_num, shuffle = False)\n",
    "\n",
    "tmp_eng, tmp_kor, tmp_eng_size, tmp_kor_size, em, km = batch_generator.make_batch(batch_size, en_length, kr_length)\n",
    "print(tmp_eng[0:5])\n",
    "print(tmp_eng_size[0:5])\n",
    "print(tmp_kor[0:5])\n",
    "print(tmp_kor_size[0:5])\n",
    "for i in range(5):\n",
    "    print(eng_indexed[i])\n",
    "#print(tmp_eng.shape)\n",
    "print(em[0:5])\n",
    "print(km[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, enc_units):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.enc_units = enc_units\n",
    "    #우리 예제에서는, 별도의 토큰 임베딩을 이용한다.\n",
    "    #self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.rnn = tf.keras.layers.RNN(\n",
    "            tf.keras.layers.LSTMCell(self.enc_units, activation = 'relu'),\n",
    "            return_sequences = True, \n",
    "            return_state = True)\n",
    "\n",
    "  def call(self, x, hidden, mask):\n",
    "    #x = self.embedding(x)\n",
    "    if mask is not None:\n",
    "      mask = tf.convert_to_tensor(mask)\n",
    "    output, *state = self.rnn(x, mask=mask, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self, batch_sz):\n",
    "    return [tf.zeros((batch_sz, self.enc_units)), tf.zeros((batch_sz, self.enc_units))]\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values, value_mask):\n",
    "    # 쿼리 은닉 상태(query hidden state)는 (batch_size, hidden size)쌍으로 이루어져 있습니다.\n",
    "    # query_with_time_axis은 (batch_size, 1, hidden size)쌍으로 이루어져 있습니다.\n",
    "    # values는 (batch_size, max_len, hidden size)쌍으로 이루어져 있습니다.\n",
    "    # 스코어(score)계산을 위해 덧셈을 수행하고자 시간 축을 확장하여 아래의 과정을 수행합니다.\n",
    "    #query_with_time_axis = tf.expand_dims(query, 1)\n",
    "    query_with_time_axis = query\n",
    "    # score는 (batch_size, max_length, 1)쌍으로 이루어져 있습니다.\n",
    "    # score를 self.V에 적용하기 때문에 마지막 축에 1을 얻습니다.\n",
    "    # self.V에 적용하기 전에 텐서는 (batch_size, max_length, units)쌍으로 이루어져 있습니다.\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(query_with_time_axis) + self.W2(values)))\n",
    "    \n",
    "    score += -1e5*tf.cast(tf.expand_dims(~value_mask, -1), dtype=score.dtype)\n",
    "    \n",
    "    # attention_weights는 (batch_size, max_length, 1)쌍으로 이루어져 있습니다. \n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    \n",
    "    # 덧셈이후 컨텍스트 벡터(context_vector)는 (batch_size, hidden_size)쌍으로 이루어져 있습니다.\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights\n",
    "  \n",
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, dec_units):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.dec_units = dec_units\n",
    "    #우리 예제에서는, 별도의 토큰 임베딩을 이용한다.\n",
    "    #self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.rnn = tf.keras.layers.RNN(\n",
    "            tf.keras.layers.LSTMCell(self.dec_units, activation = 'relu'),\n",
    "            return_sequences = True, \n",
    "            return_state = True)\n",
    "    \n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # 어텐션을 사용합니다.\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output, enc_mask):\n",
    "    \n",
    "\n",
    "    # 임베딩층을 통과한 후 x는 (batch_size, 1, embedding_dim)쌍으로 이루어져 있습니다.\n",
    "    #x = self.embedding(x)\n",
    "\n",
    "    # 컨텍스트 벡터와 임베딩 결과를 결합한 이후 x의 형태는 (batch_size, 1, embedding_dim + hidden_size)쌍으로 이루어져 있습니다.\n",
    "    #x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # 위에서 결합된 벡터를 LSTM에 전달합니다.\n",
    "    output, *state = self.rnn(x)\n",
    "    \n",
    "    # enc_output는 (batch_size, max_length, hidden_size)쌍으로 이루어져 있습니다.\n",
    "    context_vector, attention_weights = self.attention(output, enc_output, enc_mask)\n",
    "    #print(context_vector.shape)\n",
    "    # 컨텍스트 벡터와 디코더 출력 결과를 결합한 이후 x의 형태는 (batch_size, 1, hidden_size * 2)쌍으로 이루어져 있습니다.\n",
    "    output = tf.concat([tf.expand_dims(context_vector, 1), output], axis=-1)\n",
    "    \n",
    "    # output은 (batch_size * 1, hidden_size)쌍으로 이루어져 있습니다.\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output은 (batch_size, vocab)쌍으로 이루어져 있습니다.\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x284b351d848>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(num_units)\n",
    "decoder = Decoder(eng_rdict_size, num_units)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred, mask):\n",
    "  ##mask = tf.math.logical_not(tf.math.equal(real, 1))\n",
    "  ##\n",
    "  #mask = tf.constant(mask)\n",
    "  loss_ = loss_object(real, pred)\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_dir = './test_training_checkpoints3'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.1074\n",
      "Epoch 1 Batch 100 Loss 0.0885\n",
      "Epoch 1 Batch 200 Loss 0.0981\n",
      "Epoch 1 Batch 300 Loss 0.0976\n",
      "Epoch 1 Batch 400 Loss 0.1031\n",
      "Epoch 1 Batch 500 Loss 0.1031\n",
      "Epoch 1 Batch 600 Loss 0.1029\n",
      "Epoch 1 Batch 700 Loss 0.1020\n",
      "Epoch 1 Batch 800 Loss 0.1018\n",
      "Epoch 1 Batch 900 Loss 19.3271\n",
      "Epoch 1 Batch 1000 Loss 0.1007\n",
      "Epoch 1 Batch 1100 Loss 0.1003\n",
      "Epoch 1 Batch 1200 Loss 0.0997\n",
      "Epoch 1 Batch 1300 Loss 0.0994\n",
      "Epoch 1 Batch 1400 Loss 0.1180\n",
      "Epoch 1 Batch 1500 Loss 0.0985\n",
      "Epoch 1 Batch 1600 Loss 2035.6350\n",
      "Epoch 1 Batch 1700 Loss 0.5653\n",
      "Epoch 1 Batch 1800 Loss 0.0981\n",
      "Epoch 1 Batch 1900 Loss 0.0952\n",
      "Epoch 1 Batch 2000 Loss 0.0951\n",
      "Epoch 1 Batch 2100 Loss 0.0950\n",
      "Epoch 1 Batch 2200 Loss 0.0928\n",
      "Epoch 1 Batch 2300 Loss 0.0939\n",
      "Epoch 1 Batch 2400 Loss 0.1502\n",
      "Epoch 1 Batch 2500 Loss 0.0934\n",
      "Epoch 1 Batch 2600 Loss 0.0930\n",
      "Epoch 1 Batch 2700 Loss 5.6364\n",
      "Epoch 1 Batch 2800 Loss 10062.0166\n",
      "Epoch 1 Batch 2900 Loss 0.4810\n",
      "Epoch 1 Batch 3000 Loss 0.1028\n",
      "Epoch 1 Batch 3100 Loss 0.0890\n",
      "Epoch 1 Batch 3200 Loss 0.1135\n",
      "Epoch 1 Batch 3300 Loss 0.0889\n",
      "Epoch 1 Batch 3400 Loss 11.6426\n",
      "Epoch 1 Batch 3500 Loss 0.0882\n",
      "Epoch 1 Batch 3600 Loss 1.5433\n",
      "Epoch 1 Batch 3700 Loss 0.0882\n",
      "Epoch 1 Batch 3800 Loss 3.1471\n",
      "Epoch 1 Batch 3900 Loss 0.1111\n",
      "Epoch 1 Batch 4000 Loss 0.1024\n",
      "Epoch 1 Batch 4100 Loss 3.0022\n",
      "Epoch 1 Batch 4200 Loss 91.8221\n",
      "Epoch 1 Batch 4300 Loss 10.6842\n",
      "Epoch 1 Batch 4400 Loss 0.0864\n",
      "Epoch 1 Batch 4500 Loss 0.2775\n",
      "Epoch 1 Batch 4600 Loss 0.0825\n",
      "Epoch 1 Batch 4700 Loss 0.0833\n",
      "Epoch 1 Batch 4800 Loss 0.0817\n",
      "Epoch 1 Batch 4900 Loss 555.4384\n",
      "Epoch 1 Batch 5000 Loss 188.7397\n",
      "Epoch 1 Batch 5100 Loss 0.2163\n",
      "Epoch 1 Batch 5200 Loss 0.1931\n",
      "Epoch 1 Batch 5300 Loss 0.0823\n",
      "Epoch 1 Batch 5400 Loss 0.0803\n",
      "Epoch 1 Batch 5500 Loss 0.1122\n",
      "Epoch 1 Batch 5600 Loss 0.0901\n",
      "Epoch 1 Batch 5700 Loss 0.0931\n",
      "Epoch 1 Batch 5800 Loss 2610.6606\n",
      "Epoch 1 Batch 5900 Loss 0.0774\n",
      "Epoch 1 Batch 6000 Loss 0.1267\n",
      "Epoch 1 Batch 6100 Loss 0.0756\n",
      "Epoch 1 Batch 6200 Loss 0.0815\n",
      "Epoch 1 Batch 6300 Loss 0.1184\n",
      "Epoch 1 Batch 6400 Loss 10.9527\n",
      "Epoch 1 Batch 6500 Loss 6262.1943\n",
      "Epoch 1 Batch 6600 Loss 0.0757\n",
      "Epoch 1 Batch 6700 Loss 0.1482\n",
      "Epoch 1 Batch 6800 Loss 7.2527\n",
      "Epoch 1 Batch 6900 Loss 0.3105\n",
      "Epoch 1 Batch 7000 Loss 0.1387\n",
      "Epoch 1 Batch 7100 Loss 0.1336\n",
      "Epoch 1 Batch 7200 Loss 1.2997\n",
      "input: despite its relatively understated styling , the UNK has a UNK , UNK silhouette and vivid performance that gives it real presence on the road . \n",
      "targ: 상대적 으로 저 어 평가 되 ㄴ 헤어스타일 에 도 불구 하 고 , 모 나로 는 세련 되 고 , 뒤 로 넘기 ㄴ UNK 과 생생 하 ㄴ 효능 이 있 어 거리 에서 실제 로 존재 감 이 드러나 게 하 ㄴ다 . \n",
      "output: 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 하 \n",
      "input: drop calves [ UNK , lambs ] \n",
      "targ: 송아지 [ 망아지 , 새끼 양 ] 를 낳 다 . \n",
      "output: 은 ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ \n",
      "input: this is the third UNK single . \n",
      "targ: 지금 당나귀 가 태어나 었 는데 , 새끼 당나귀 가 3 마리나 되 ㄴ다 \n",
      "output: 은 ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ ㄴ \n",
      "Epoch 1 Loss 1337216.3750\n",
      "Time taken for 1 epoch 2302.8986043930054 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.0752\n",
      "Epoch 2 Batch 100 Loss 0.0754\n",
      "Epoch 2 Batch 200 Loss 0.0739\n",
      "Epoch 2 Batch 300 Loss 0.0741\n",
      "Epoch 2 Batch 400 Loss 0.0753\n",
      "Epoch 2 Batch 500 Loss 0.0736\n",
      "Epoch 2 Batch 600 Loss 0.0750\n",
      "Epoch 2 Batch 700 Loss 0.0723\n",
      "Epoch 2 Batch 800 Loss 0.0732\n",
      "Epoch 2 Batch 900 Loss 0.0739\n",
      "Epoch 2 Batch 1000 Loss 0.0729\n",
      "Epoch 2 Batch 1100 Loss 0.0723\n",
      "Epoch 2 Batch 1200 Loss 0.0731\n",
      "Epoch 2 Batch 1300 Loss 0.0718\n",
      "Epoch 2 Batch 1400 Loss 0.0710\n",
      "Epoch 2 Batch 1500 Loss 0.0703\n",
      "Epoch 2 Batch 1600 Loss 0.0715\n",
      "Epoch 2 Batch 1700 Loss 0.3963\n",
      "Epoch 2 Batch 1800 Loss 0.0745\n",
      "Epoch 2 Batch 1900 Loss 0.0705\n",
      "Epoch 2 Batch 2000 Loss 0.0696\n",
      "Epoch 2 Batch 2100 Loss 0.0686\n",
      "Epoch 2 Batch 2200 Loss 0.0707\n",
      "Epoch 2 Batch 2300 Loss 0.6636\n",
      "Epoch 2 Batch 2400 Loss 0.4845\n",
      "Epoch 2 Batch 2500 Loss 0.0694\n",
      "Epoch 2 Batch 2600 Loss 0.0685\n",
      "Epoch 2 Batch 2700 Loss 747.2885\n",
      "Epoch 2 Batch 2800 Loss 0.0686\n",
      "Epoch 2 Batch 2900 Loss 0.3499\n",
      "Epoch 2 Batch 3000 Loss 0.0677\n",
      "Epoch 2 Batch 3100 Loss 4428.9937\n",
      "Epoch 2 Batch 3200 Loss 0.0829\n",
      "Epoch 2 Batch 3300 Loss 0.0675\n",
      "Epoch 2 Batch 3400 Loss 1107.1729\n",
      "Epoch 2 Batch 3500 Loss 0.0658\n",
      "Epoch 2 Batch 3600 Loss 0.8589\n",
      "Epoch 2 Batch 3700 Loss 0.0675\n",
      "Epoch 2 Batch 3800 Loss 1.4287\n",
      "Epoch 2 Batch 3900 Loss 0.0669\n",
      "Epoch 2 Batch 4000 Loss 0.0648\n",
      "Epoch 2 Batch 4100 Loss 0.1079\n",
      "Epoch 2 Batch 4200 Loss 2004.0988\n",
      "Epoch 2 Batch 4300 Loss 6.7573\n",
      "Epoch 2 Batch 4400 Loss 2.5611\n",
      "Epoch 2 Batch 4500 Loss 0.0653\n",
      "Epoch 2 Batch 4600 Loss 0.0643\n",
      "Epoch 2 Batch 4700 Loss 0.0638\n",
      "Epoch 2 Batch 4800 Loss 0.0722\n",
      "Epoch 2 Batch 4900 Loss 0.1906\n",
      "Epoch 2 Batch 5000 Loss 117.1118\n",
      "Epoch 2 Batch 5100 Loss 0.1562\n",
      "Epoch 2 Batch 5200 Loss 3.4585\n",
      "Epoch 2 Batch 5300 Loss 0.0984\n",
      "Epoch 2 Batch 5400 Loss 0.0684\n",
      "Epoch 2 Batch 5500 Loss 0.0666\n",
      "Epoch 2 Batch 5600 Loss 0.0675\n",
      "Epoch 2 Batch 5700 Loss 0.0984\n",
      "Epoch 2 Batch 5800 Loss 0.2740\n",
      "Epoch 2 Batch 5900 Loss 331.8299\n",
      "Epoch 2 Batch 6000 Loss 0.0627\n",
      "Epoch 2 Batch 6100 Loss 0.0945\n",
      "Epoch 2 Batch 6200 Loss 20.6513\n",
      "Epoch 2 Batch 6300 Loss 0.0623\n",
      "Epoch 2 Batch 6400 Loss 2.0105\n",
      "Epoch 2 Batch 6500 Loss 864.7211\n",
      "Epoch 2 Batch 6600 Loss 0.0631\n",
      "Epoch 2 Batch 6700 Loss 0.0636\n",
      "Epoch 2 Batch 6800 Loss 0.7296\n",
      "Epoch 2 Batch 6900 Loss 0.1284\n",
      "Epoch 2 Batch 7000 Loss 0.0832\n",
      "Epoch 2 Batch 7100 Loss 0.0760\n",
      "Epoch 2 Batch 7200 Loss 0.0644\n",
      "input: the control group of the experiment \n",
      "targ: 그 실험 의 통제 집단 . \n",
      "output: 이 \n",
      "input: the shortest day in the year \n",
      "targ: 일 년 중 에 가장 짧 은 날 . \n",
      "output: 이 \n",
      "input: the man is wheeling the machine into the repair shop . \n",
      "targ: 남자 가 기계 를 수리 점 으로 가져가 고 있 다 . \n",
      "output: 이 \n",
      "Epoch 2 Loss 186.1847\n",
      "Time taken for 1 epoch 1928.7612743377686 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.0616\n",
      "Epoch 3 Batch 100 Loss 0.0608\n",
      "Epoch 3 Batch 200 Loss 0.0599\n",
      "Epoch 3 Batch 300 Loss 0.0610\n",
      "Epoch 3 Batch 400 Loss 0.0595\n",
      "Epoch 3 Batch 500 Loss 0.0604\n",
      "Epoch 3 Batch 600 Loss 0.0607\n",
      "Epoch 3 Batch 700 Loss 0.0600\n",
      "Epoch 3 Batch 800 Loss 0.0609\n",
      "Epoch 3 Batch 900 Loss 0.0630\n",
      "Epoch 3 Batch 1000 Loss 0.0605\n",
      "Epoch 3 Batch 1100 Loss 0.0611\n",
      "Epoch 3 Batch 1200 Loss 0.0800\n",
      "Epoch 3 Batch 1300 Loss 0.0593\n",
      "Epoch 3 Batch 1400 Loss 0.0605\n",
      "Epoch 3 Batch 1500 Loss 0.0606\n",
      "Epoch 3 Batch 1600 Loss 0.0614\n",
      "Epoch 3 Batch 1700 Loss 0.0614\n",
      "Epoch 3 Batch 1800 Loss 0.0620\n",
      "Epoch 3 Batch 1900 Loss 0.0620\n",
      "Epoch 3 Batch 2000 Loss 0.0612\n",
      "Epoch 3 Batch 2100 Loss 0.0591\n",
      "Epoch 3 Batch 2200 Loss 0.0620\n",
      "Epoch 3 Batch 2300 Loss 0.7027\n",
      "Epoch 3 Batch 2400 Loss 0.1341\n",
      "Epoch 3 Batch 2500 Loss 0.0613\n",
      "Epoch 3 Batch 2600 Loss 0.0588\n",
      "Epoch 3 Batch 2700 Loss 64.4515\n",
      "Epoch 3 Batch 2800 Loss 0.0588\n",
      "Epoch 3 Batch 2900 Loss 0.4196\n",
      "Epoch 3 Batch 3000 Loss 0.0597\n",
      "Epoch 3 Batch 3100 Loss 0.0707\n",
      "Epoch 3 Batch 3200 Loss 0.0587\n",
      "Epoch 3 Batch 3300 Loss 0.0623\n",
      "Epoch 3 Batch 3400 Loss 333.4055\n",
      "Epoch 3 Batch 3500 Loss 0.0587\n",
      "Epoch 3 Batch 3600 Loss 0.0580\n",
      "Epoch 3 Batch 3700 Loss 0.0591\n",
      "Epoch 3 Batch 3800 Loss 0.0587\n",
      "Epoch 3 Batch 3900 Loss 0.0605\n",
      "Epoch 3 Batch 4000 Loss 0.0586\n",
      "Epoch 3 Batch 4100 Loss 0.0594\n",
      "Epoch 3 Batch 4200 Loss 96.3296\n",
      "Epoch 3 Batch 4300 Loss 1.3637\n",
      "Epoch 3 Batch 4400 Loss 0.1661\n",
      "Epoch 3 Batch 4500 Loss 0.0722\n",
      "Epoch 3 Batch 4600 Loss 0.0577\n",
      "Epoch 3 Batch 4700 Loss 0.1635\n",
      "Epoch 3 Batch 4800 Loss 0.1277\n",
      "Epoch 3 Batch 4900 Loss 0.0694\n",
      "Epoch 3 Batch 5000 Loss 0.0586\n",
      "Epoch 3 Batch 5100 Loss 0.0615\n",
      "Epoch 3 Batch 5200 Loss 0.1970\n",
      "Epoch 3 Batch 5300 Loss 0.0619\n",
      "Epoch 3 Batch 5400 Loss 0.0569\n",
      "Epoch 3 Batch 5500 Loss 0.0597\n",
      "Epoch 3 Batch 5600 Loss 0.0585\n",
      "Epoch 3 Batch 5700 Loss 0.0609\n",
      "Epoch 3 Batch 5800 Loss 0.0581\n",
      "Epoch 3 Batch 5900 Loss 6.9690\n",
      "Epoch 3 Batch 6000 Loss 0.0581\n",
      "Epoch 3 Batch 6100 Loss 32.2455\n",
      "Epoch 3 Batch 6200 Loss 0.8213\n",
      "Epoch 3 Batch 6300 Loss 0.0579\n",
      "Epoch 3 Batch 6400 Loss 0.0575\n",
      "Epoch 3 Batch 6500 Loss 40.5528\n",
      "Epoch 3 Batch 6600 Loss 0.0833\n",
      "Epoch 3 Batch 6700 Loss 0.0591\n",
      "Epoch 3 Batch 6800 Loss 0.0998\n",
      "Epoch 3 Batch 6900 Loss 0.0995\n",
      "Epoch 3 Batch 7000 Loss 0.0577\n",
      "Epoch 3 Batch 7100 Loss 0.0665\n",
      "Epoch 3 Batch 7200 Loss 0.0598\n",
      "input: he 's bright and kind , and i like him a lot . \n",
      "targ: 그 는 똑똑 하 고 친절 하 어서 전 그릏 ㄹ 매우 좋아하 어요 . \n",
      "output: 그 는 \n",
      "input: yes , and it was a lot of fun . \n",
      "targ: 예 , 참 재미있 었 어요 . \n",
      "output: 그 는 \n",
      "input: and i will pay tribute to him as a partner and as a friend . \n",
      "targ: 이제 저 는 그르 ㄹ 파트너 이자 친구 로서 경의 를 표하 고자 하 ㅂ니다 . \n",
      "output: 그 는 \n",
      "Epoch 3 Loss 8.9996\n",
      "Time taken for 1 epoch 1923.8222210407257 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 0 Loss 0.0545\n",
      "Epoch 4 Batch 100 Loss 0.0559\n",
      "Epoch 4 Batch 200 Loss 0.0544\n",
      "Epoch 4 Batch 300 Loss 0.0547\n",
      "Epoch 4 Batch 400 Loss 0.0525\n",
      "Epoch 4 Batch 500 Loss 0.0560\n",
      "Epoch 4 Batch 600 Loss 0.0560\n",
      "Epoch 4 Batch 700 Loss 0.0530\n",
      "Epoch 4 Batch 800 Loss 0.0553\n",
      "Epoch 4 Batch 900 Loss 0.0577\n",
      "Epoch 4 Batch 1000 Loss 0.0541\n",
      "Epoch 4 Batch 1100 Loss 0.0543\n",
      "Epoch 4 Batch 1200 Loss 0.0548\n",
      "Epoch 4 Batch 1300 Loss 0.0538\n",
      "Epoch 4 Batch 1400 Loss 0.0526\n",
      "Epoch 4 Batch 1500 Loss 0.0541\n",
      "Epoch 4 Batch 1600 Loss 0.0550\n",
      "Epoch 4 Batch 1700 Loss 0.0540\n",
      "Epoch 4 Batch 1800 Loss 0.0534\n",
      "Epoch 4 Batch 1900 Loss 0.0535\n",
      "Epoch 4 Batch 2000 Loss 0.0558\n",
      "Epoch 4 Batch 2100 Loss 0.0514\n",
      "Epoch 4 Batch 2200 Loss 0.0553\n",
      "Epoch 4 Batch 2300 Loss 0.0800\n",
      "Epoch 4 Batch 2400 Loss 0.0532\n",
      "Epoch 4 Batch 2500 Loss 0.0552\n",
      "Epoch 4 Batch 2600 Loss 0.0519\n",
      "Epoch 4 Batch 2700 Loss 0.0549\n",
      "Epoch 4 Batch 2800 Loss 0.0512\n",
      "Epoch 4 Batch 2900 Loss 0.0672\n",
      "Epoch 4 Batch 3000 Loss 0.0536\n",
      "Epoch 4 Batch 3100 Loss 0.0669\n",
      "Epoch 4 Batch 3200 Loss 0.0536\n",
      "Epoch 4 Batch 3300 Loss 0.0544\n",
      "Epoch 4 Batch 3400 Loss 0.0552\n",
      "Epoch 4 Batch 3500 Loss 0.0519\n",
      "Epoch 4 Batch 3600 Loss 0.0479\n",
      "Epoch 4 Batch 3700 Loss 0.0801\n",
      "Epoch 4 Batch 3800 Loss 0.0529\n",
      "Epoch 4 Batch 3900 Loss 0.0547\n",
      "Epoch 4 Batch 4000 Loss 0.0538\n",
      "Epoch 4 Batch 4100 Loss 0.2333\n",
      "Epoch 4 Batch 4200 Loss 0.0497\n",
      "Epoch 4 Batch 4300 Loss 0.0661\n",
      "Epoch 4 Batch 4400 Loss 0.0544\n",
      "Epoch 4 Batch 4500 Loss 0.0533\n",
      "Epoch 4 Batch 4600 Loss 0.0489\n",
      "Epoch 4 Batch 4700 Loss 0.0515\n",
      "Epoch 4 Batch 4800 Loss 0.0696\n",
      "Epoch 4 Batch 4900 Loss 0.0506\n",
      "Epoch 4 Batch 5000 Loss 0.0738\n",
      "Epoch 4 Batch 5100 Loss 0.0518\n",
      "Epoch 4 Batch 5200 Loss 0.0516\n",
      "Epoch 4 Batch 5300 Loss 0.0526\n",
      "Epoch 4 Batch 5400 Loss 0.0520\n",
      "Epoch 4 Batch 5500 Loss 0.0561\n",
      "Epoch 4 Batch 5600 Loss 0.0515\n",
      "Epoch 4 Batch 5700 Loss 0.0526\n",
      "Epoch 4 Batch 5800 Loss 0.0481\n",
      "Epoch 4 Batch 5900 Loss 0.0521\n",
      "Epoch 4 Batch 6000 Loss 0.0535\n",
      "Epoch 4 Batch 6100 Loss 0.0515\n",
      "Epoch 4 Batch 6200 Loss 0.0502\n",
      "Epoch 4 Batch 6300 Loss 0.0536\n",
      "Epoch 4 Batch 6400 Loss 0.0535\n",
      "Epoch 4 Batch 6500 Loss 0.2620\n",
      "Epoch 4 Batch 6600 Loss 0.0517\n",
      "Epoch 4 Batch 6700 Loss 0.0516\n",
      "Epoch 4 Batch 6800 Loss 0.0528\n",
      "Epoch 4 Batch 6900 Loss 0.0525\n",
      "Epoch 4 Batch 7000 Loss 0.0513\n",
      "Epoch 4 Batch 7100 Loss 0.0529\n",
      "Epoch 4 Batch 7200 Loss 0.0525\n",
      "input: a man presented himself as a witness . \n",
      "targ: 한 남자 가 증인 으로 출두 하 었 다 \n",
      "output: 그 는 것 이 다 . \n",
      "input: a worker is using a screwdriver . \n",
      "targ: 일꾼 이 스크루 드라이버 를 사용 하 고 있 다 . \n",
      "output: 그 는 것 이 다 . \n",
      "input: that 's a stiff price . or it costs me an arm and a leg . \n",
      "targ: 엄청나 게 비싸 군요 \n",
      "output: 그 는 것 이 다 . \n",
      "Epoch 4 Loss 0.2331\n",
      "Time taken for 1 epoch 1892.5643095970154 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0463\n",
      "Epoch 5 Batch 100 Loss 0.0471\n",
      "Epoch 5 Batch 200 Loss 0.0448\n",
      "Epoch 5 Batch 300 Loss 0.0449\n",
      "Epoch 5 Batch 400 Loss 0.0445\n",
      "Epoch 5 Batch 500 Loss 0.0483\n",
      "Epoch 5 Batch 600 Loss 0.0494\n",
      "Epoch 5 Batch 700 Loss 0.0462\n",
      "Epoch 5 Batch 800 Loss 0.0468\n",
      "Epoch 5 Batch 900 Loss 0.0495\n",
      "Epoch 5 Batch 1000 Loss 0.0449\n",
      "Epoch 5 Batch 1100 Loss 0.0474\n",
      "Epoch 5 Batch 1200 Loss 0.0481\n",
      "Epoch 5 Batch 1300 Loss 0.0490\n",
      "Epoch 5 Batch 1400 Loss 0.0459\n",
      "Epoch 5 Batch 1500 Loss 0.0474\n",
      "Epoch 5 Batch 1600 Loss 0.0436\n",
      "Epoch 5 Batch 1700 Loss 0.0466\n",
      "Epoch 5 Batch 1800 Loss 0.0432\n",
      "Epoch 5 Batch 1900 Loss 0.0451\n",
      "Epoch 5 Batch 2000 Loss 0.0501\n",
      "Epoch 5 Batch 2100 Loss 0.0449\n",
      "Epoch 5 Batch 2200 Loss 0.0469\n",
      "Epoch 5 Batch 2300 Loss 0.0428\n",
      "Epoch 5 Batch 2400 Loss 0.0454\n",
      "Epoch 5 Batch 2500 Loss 0.0469\n",
      "Epoch 5 Batch 2600 Loss 0.0450\n",
      "Epoch 5 Batch 2700 Loss 0.0488\n",
      "Epoch 5 Batch 2800 Loss 0.0448\n",
      "Epoch 5 Batch 2900 Loss 0.0492\n",
      "Epoch 5 Batch 3000 Loss 0.0455\n",
      "Epoch 5 Batch 3100 Loss 0.0498\n",
      "Epoch 5 Batch 3200 Loss 0.0473\n",
      "Epoch 5 Batch 3300 Loss 0.0447\n",
      "Epoch 5 Batch 3400 Loss 0.0461\n",
      "Epoch 5 Batch 3500 Loss 0.0477\n",
      "Epoch 5 Batch 3600 Loss 0.0456\n",
      "Epoch 5 Batch 3700 Loss 0.0493\n",
      "Epoch 5 Batch 3800 Loss 0.0489\n",
      "Epoch 5 Batch 3900 Loss 0.0511\n",
      "Epoch 5 Batch 4000 Loss 0.0517\n",
      "Epoch 5 Batch 4100 Loss 0.0477\n",
      "Epoch 5 Batch 4200 Loss 0.0451\n",
      "Epoch 5 Batch 4300 Loss 0.0470\n",
      "Epoch 5 Batch 4400 Loss 0.0446\n",
      "Epoch 5 Batch 4500 Loss 0.0500\n",
      "Epoch 5 Batch 4600 Loss 0.0438\n",
      "Epoch 5 Batch 4700 Loss 0.0426\n",
      "Epoch 5 Batch 4800 Loss 0.0456\n",
      "Epoch 5 Batch 4900 Loss 0.0442\n",
      "Epoch 5 Batch 5000 Loss 0.0459\n",
      "Epoch 5 Batch 5100 Loss 0.0458\n",
      "Epoch 5 Batch 5200 Loss 0.0463\n",
      "Epoch 5 Batch 5300 Loss 0.0498\n",
      "Epoch 5 Batch 5400 Loss 0.0483\n",
      "Epoch 5 Batch 5500 Loss 0.0512\n",
      "Epoch 5 Batch 5600 Loss 0.0458\n",
      "Epoch 5 Batch 5700 Loss 0.0474\n",
      "Epoch 5 Batch 5800 Loss 0.0450\n",
      "Epoch 5 Batch 5900 Loss 0.0487\n",
      "Epoch 5 Batch 6000 Loss 0.0468\n",
      "Epoch 5 Batch 6100 Loss 0.0452\n",
      "Epoch 5 Batch 6200 Loss 0.0465\n",
      "Epoch 5 Batch 6300 Loss 0.0505\n",
      "Epoch 5 Batch 6400 Loss 0.0509\n",
      "Epoch 5 Batch 6500 Loss 0.0453\n",
      "Epoch 5 Batch 6600 Loss 0.0446\n",
      "Epoch 5 Batch 6700 Loss 0.0465\n",
      "Epoch 5 Batch 6800 Loss 0.0462\n",
      "Epoch 5 Batch 6900 Loss 0.0487\n",
      "Epoch 5 Batch 7000 Loss 0.0471\n",
      "Epoch 5 Batch 7100 Loss 0.0461\n",
      "Epoch 5 Batch 7200 Loss 0.0485\n",
      "input: the retry count must be a positive , UNK integer number . \n",
      "targ: 재 시도 횟수 는 양의 정수 여야 하 ㅂ니다 . \n",
      "output: 그 는 것 이 다 . \n",
      "input: it ’ s worked out great for nine years . \n",
      "targ: 지나 ㄴ 9 년 간 이 방식 은 성공적 이 었 습니다 . \n",
      "output: 그 는 것 이 다 . \n",
      "input: the UNK UNK news time is UNK . \n",
      "targ: UNK UNK - UNK 뉴스 시간 은 9 시 20 분 이 ㅂ니다 . \n",
      "output: 그 는 것 이 다 . \n",
      "Epoch 5 Loss 0.0469\n",
      "Time taken for 1 epoch 1922.3987877368927 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0406\n",
      "Epoch 6 Batch 100 Loss 0.0417\n",
      "Epoch 6 Batch 200 Loss 0.0400\n",
      "Epoch 6 Batch 300 Loss 0.0402\n",
      "Epoch 6 Batch 400 Loss 0.0435\n",
      "Epoch 6 Batch 500 Loss 0.0418\n",
      "Epoch 6 Batch 600 Loss 0.0440\n",
      "Epoch 6 Batch 700 Loss 0.0434\n",
      "Epoch 6 Batch 800 Loss 0.0416\n",
      "Epoch 6 Batch 900 Loss 0.0425\n",
      "Epoch 6 Batch 1000 Loss 0.0436\n",
      "Epoch 6 Batch 1100 Loss 0.0461\n",
      "Epoch 6 Batch 1200 Loss 0.0444\n",
      "Epoch 6 Batch 1300 Loss 0.0459\n",
      "Epoch 6 Batch 1400 Loss 0.0460\n",
      "Epoch 6 Batch 1500 Loss 0.0453\n",
      "Epoch 6 Batch 1600 Loss 0.0401\n",
      "Epoch 6 Batch 1700 Loss 0.0409\n",
      "Epoch 6 Batch 1800 Loss 0.0412\n",
      "Epoch 6 Batch 1900 Loss 0.0424\n",
      "Epoch 6 Batch 2000 Loss 0.0436\n",
      "Epoch 6 Batch 2100 Loss 0.0424\n",
      "Epoch 6 Batch 2200 Loss 0.0458\n",
      "Epoch 6 Batch 2300 Loss 0.0400\n",
      "Epoch 6 Batch 2400 Loss 0.0404\n",
      "Epoch 6 Batch 2500 Loss 0.0444\n",
      "Epoch 6 Batch 2600 Loss 0.0434\n",
      "Epoch 6 Batch 2700 Loss 0.0443\n",
      "Epoch 6 Batch 2800 Loss 0.0442\n",
      "Epoch 6 Batch 2900 Loss 0.0460\n",
      "Epoch 6 Batch 3000 Loss 0.0441\n",
      "Epoch 6 Batch 3100 Loss 0.0451\n",
      "Epoch 6 Batch 3200 Loss 0.0437\n",
      "Epoch 6 Batch 3300 Loss 0.0411\n",
      "Epoch 6 Batch 3400 Loss 0.0412\n",
      "Epoch 6 Batch 3500 Loss 0.0434\n",
      "Epoch 6 Batch 3600 Loss 0.0422\n",
      "Epoch 6 Batch 3700 Loss 0.0443\n",
      "Epoch 6 Batch 3800 Loss 0.0447\n",
      "Epoch 6 Batch 3900 Loss 0.0474\n",
      "Epoch 6 Batch 4000 Loss 0.0475\n",
      "Epoch 6 Batch 4100 Loss 0.0408\n",
      "Epoch 6 Batch 4200 Loss 0.0446\n",
      "Epoch 6 Batch 4300 Loss 0.0442\n",
      "Epoch 6 Batch 4400 Loss 0.0424\n",
      "Epoch 6 Batch 4500 Loss 0.0466\n",
      "Epoch 6 Batch 4600 Loss 0.0436\n",
      "Epoch 6 Batch 4700 Loss 0.0373\n",
      "Epoch 6 Batch 4800 Loss 0.0395\n",
      "Epoch 6 Batch 4900 Loss 0.0458\n",
      "Epoch 6 Batch 5000 Loss 0.0423\n",
      "Epoch 6 Batch 5100 Loss 0.0454\n",
      "Epoch 6 Batch 5200 Loss 0.0417\n",
      "Epoch 6 Batch 5300 Loss 0.0455\n",
      "Epoch 6 Batch 5400 Loss 0.0423\n",
      "Epoch 6 Batch 5500 Loss 0.0484\n",
      "Epoch 6 Batch 5600 Loss 0.0454\n",
      "Epoch 6 Batch 5700 Loss 0.0469\n",
      "Epoch 6 Batch 5800 Loss 0.0436\n",
      "Epoch 6 Batch 5900 Loss 0.0452\n",
      "Epoch 6 Batch 6000 Loss 0.0435\n",
      "Epoch 6 Batch 6100 Loss 0.0447\n",
      "Epoch 6 Batch 6200 Loss 0.0434\n",
      "Epoch 6 Batch 6300 Loss 0.0458\n",
      "Epoch 6 Batch 6400 Loss 0.0465\n",
      "Epoch 6 Batch 6500 Loss 0.0440\n",
      "Epoch 6 Batch 6600 Loss 0.0416\n",
      "Epoch 6 Batch 6700 Loss 0.0449\n",
      "Epoch 6 Batch 6800 Loss 0.0440\n",
      "Epoch 6 Batch 6900 Loss 0.0452\n",
      "Epoch 6 Batch 7000 Loss 0.0456\n",
      "Epoch 6 Batch 7100 Loss 0.0414\n",
      "Epoch 6 Batch 7200 Loss 0.0451\n",
      "input: the theme of the contest is “ how big this country is . ” \n",
      "targ: 대회 의 주제 는 “ 이 도시 가 얼마나 크 ㄴ가 ” 이 다 . \n",
      "output: 그 는 것 이 다 . \n",
      "input: it is rumored that the house is haunted . \n",
      "targ: 그 집 에 는 유령 이 나오 ㄴ다는 소문 이 있 다 \n",
      "output: 그 는 것 이 다 . \n",
      "input: as is the king , so is the people . \n",
      "targ: UNK 이 맑 아야 UNK 이 맑 다 \n",
      "output: 그 는 것 이 다 . \n",
      "Epoch 6 Loss 0.0432\n",
      "Time taken for 1 epoch 1930.3286638259888 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0363\n",
      "Epoch 7 Batch 100 Loss 0.0395\n",
      "Epoch 7 Batch 200 Loss 0.0373\n",
      "Epoch 7 Batch 300 Loss 0.0365\n",
      "Epoch 7 Batch 400 Loss 0.0405\n",
      "Epoch 7 Batch 500 Loss 0.0372\n",
      "Epoch 7 Batch 600 Loss 0.0398\n",
      "Epoch 7 Batch 700 Loss 0.0393\n",
      "Epoch 7 Batch 800 Loss 0.0408\n",
      "Epoch 7 Batch 900 Loss 0.0378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 1000 Loss 0.0415\n",
      "Epoch 7 Batch 1100 Loss 0.0438\n",
      "Epoch 7 Batch 1200 Loss 0.0404\n",
      "Epoch 7 Batch 1300 Loss 0.0391\n",
      "Epoch 7 Batch 1400 Loss 0.0436\n",
      "Epoch 7 Batch 1500 Loss 0.0427\n",
      "Epoch 7 Batch 1600 Loss 0.0384\n",
      "Epoch 7 Batch 1700 Loss 0.0376\n",
      "Epoch 7 Batch 1800 Loss 0.0401\n",
      "Epoch 7 Batch 1900 Loss 0.0422\n",
      "Epoch 7 Batch 2000 Loss 0.0399\n",
      "Epoch 7 Batch 2100 Loss 0.0405\n",
      "Epoch 7 Batch 2200 Loss 0.0422\n",
      "Epoch 7 Batch 2300 Loss 0.0395\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-909342727891>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;31m#print(eng_mask)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meng_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkor_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meng_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkor_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "N_CANDIDATE = 256\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_emb, dec_emb, enc_mask, dec_mask, BATCH_SIZE,):\n",
    "  loss = 0\n",
    "  enc_hidden = encoder.initialize_hidden_state(BATCH_SIZE)\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_inp = tf.nn.embedding_lookup(enc_emb, inp)\n",
    "    enc_output, enc_hidden = encoder(enc_inp, enc_hidden, enc_mask)\n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([1] * BATCH_SIZE, 1)\n",
    "    candidate_idx = np.arange(eng_rdict_size)\n",
    "    np.random.shuffle(candidate_idx)\n",
    "    candidate_mask = np.zeros([1, eng_rdict_size])\n",
    "    candidate_mask[0, candidate_idx[:N_CANDIDATE]] = 1.0\n",
    "    predict_mask = tf.one_hot(tf.reshape(targ, [-1]), eng_rdict_size)\n",
    "    predict_mask = tf.math.reduce_max(predict_mask, axis = 0, keepdims=True)\n",
    "    predict_mask = tf.math.maximum(predict_mask, candidate_mask)\n",
    "    \n",
    "    # 교사 강요(teacher forcing) - 다음 입력으로 타겟을 피딩(feeding)합니다.\n",
    "    for t in range(0, targ.shape[1]):\n",
    "      dec_input = tf.nn.embedding_lookup(dec_emb, dec_input)\n",
    "      # enc_output를 디코더에 전달합니다.\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output, enc_mask)\n",
    "      \n",
    "      predictions *= predict_mask\n",
    "      \n",
    "      #print(predictions[0, :10])\n",
    "      #assert(False)\n",
    "      loss += loss_function(targ[:, t], predictions, dec_mask[:, t])\n",
    "\n",
    "      # 교사 강요(teacher forcing)를 사용합니다.\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "      \n",
    "\n",
    "  batch_loss = (loss / tf.reduce_sum( tf.cast(targ != 1, dtype = tf.float32) ) )\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss\n",
    "\n",
    "\n",
    "def evaluate(inp, enc_emb, dec_emb, enc_mask, dec_mask, BATCH_SIZE):\n",
    "\n",
    "  enc_inp = tf.nn.embedding_lookup(enc_emb, inp)\n",
    "  enc_hidden = encoder.initialize_hidden_state(BATCH_SIZE)\n",
    "  enc_output, enc_hidden = encoder(enc_inp, enc_hidden, enc_mask)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "\n",
    "  dec_input = tf.expand_dims([1] * BATCH_SIZE, 1)\n",
    "\n",
    "  outputs = []\n",
    "  attention_weights = []\n",
    "  # 교사 강요(teacher forcing) - 다음 입력으로 타겟을 피딩(feeding)합니다.\n",
    "  for t in range(kr_length):\n",
    "    dec_input = tf.nn.embedding_lookup(dec_emb, dec_input)\n",
    "    # enc_output를 디코더에 전달합니다.\n",
    "    predictions, dec_hidden, attention_weight = decoder(dec_input, dec_hidden, enc_output, enc_mask)\n",
    "\n",
    "    attention_weights.append(attention_weight)\n",
    "    #loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "    # 교사 강요(teacher forcing)를 사용합니다.\n",
    "    dec_input = tf.argmax(predictions, axis=-1)\n",
    "    outputs.append(dec_input)\n",
    "    dec_input = tf.expand_dims(dec_input, 1)\n",
    "    \n",
    "  outputs = np.stack(outputs, axis=1)\n",
    "  attention_weights = np.stack(attention_weights, axis=1)\n",
    "  return outputs, attention_weights\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "batch_generator = BatchGenerator(eng_indexed, kor_indexed, totalnum = sentences_num, shuffle = False)\n",
    "\n",
    "#tmp_eng, tmp_kor, tmp_eng_size, tmp_kor_size = batch_generator.make_batch(batch_size, en_length, kr_length)\n",
    "steps_per_epoch = sentences_num // batch_size + 1\n",
    "\n",
    "#prepare early stopping\n",
    "best_loss = 1000\n",
    "tol = 9999\n",
    "miss_count = 0\n",
    "#annealing_count = 4\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  total_loss = 0\n",
    "\n",
    "  \n",
    "  for batch in range(steps_per_epoch):\n",
    "    inp, targ, _, _, eng_mask, kor_mask= batch_generator.make_batch(batch_size, en_length, kr_length)\n",
    "    \n",
    "    #print(eng_mask)\n",
    "    batch_loss = train_step(inp, targ, eng_embedding, kor_embedding, eng_mask, kor_mask, batch_size)\n",
    "    \n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  outputs , attention_weights = evaluate(inp[:3], eng_embedding, kor_embedding, eng_mask[:3], kor_mask[:3], 3)\n",
    "  \n",
    "  inp_sentences = token_to_eng(inp[:3])\n",
    "  targ_sentences = token_to_kor(targ[:3])\n",
    "  output_sentences = token_to_kor(outputs[:3])\n",
    "  \n",
    "  for i in range(3):\n",
    "    print(\"input:\", inp_sentences[i])\n",
    "    print(\"targ:\", targ_sentences[i])\n",
    "    print(\"output:\", output_sentences[i])\n",
    "  \n",
    "  \n",
    "  \n",
    "  # 에포크가 2번 실행될때마다 모델 저장 (체크포인트)\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "  \n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  \n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "  if total_loss / steps_per_epoch < best_loss:\n",
    "    best_loss = total_loss / steps_per_epoch\n",
    "    miss_count = 0\n",
    "  else:\n",
    "    miss_count += 1\n",
    "    if miss_count == tol:\n",
    "      print(\"early stopping activated...\")\n",
    "      break\n",
    "    \n",
    "      \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: i 'll be back after canceling my appointment . \n",
      "targ: 약속 을 취소 하 고 다시 오 ㄹ게 . \n",
      "output: 는 것 이 다 . \n",
      "input: i think she has a doctor 's appointment . \n",
      "targ: 그녀 는 의사 와 진찰 약속 이 있 는 것 같 습니다 . \n",
      "output: 는 것 같 아요 . \n",
      "input: he is sure to be appointed . or his appointment is certain . \n",
      "targ: 그 가 임명 되 는 것 은 확실 하 다 \n",
      "output: 는 것 이 다 . \n",
      "input: i just need to pack a towel . \n",
      "targ: 수건 하나 만 싸 면 되 어요 . \n",
      "output: 는 것 이 필요 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 하 어야 \n",
      "input: i smoke about a pack of cigarettes a day . \n",
      "targ: 전 하루 에 한 갑 정도 피우 ㅂ니다 . \n",
      "output: 는 하루 에 관하 ㄴ 하루 에 관하 ㄴ 하루 에 관하 ㄴ 하루 에 관하 ㄴ 하루 에 관하 ㄴ 하루 에 관하 ㄴ 하루 에 관하 ㄴ 하루 에 관하 ㄴ 하루 에 관하 ㄴ 하루 에 관하 ㄴ 하루 에 관하 ㄴ 하루 에 관하 ㄴ 하루 에 관하 ㄴ 하루 에 관하 ㄴ 하루 에 관하 \n"
     ]
    }
   ],
   "source": [
    "outputs , attention_weights = evaluate(inp[:5], eng_embedding, kor_embedding, eng_mask[:5], kor_mask[:5], 5)\n",
    "\n",
    "inp_sentences = token_to_eng(inp[:5])\n",
    "targ_sentences = token_to_kor(targ[:5])\n",
    "output_sentences = token_to_kor(outputs[:5])\n",
    "\n",
    "for i in range(5):\n",
    "  print(\"input:\", inp_sentences[i])\n",
    "  print(\"targ:\", targ_sentences[i])\n",
    "  print(\"output:\", output_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.initialize_hidden_state(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 60)\n",
      "(16, 60, 60, 1)\n",
      "[ 11  19 106   6   8  24  11 106   9   4  22  34  40 239  29   3  42   0\n",
      "  43  11 106   9   4  22  34  40 239  29   3  42   0  43  11 106   9   4\n",
      "  22  34  40 239  29   3  42   0  43  11 106   9   4  22  34  40 239  29\n",
      "   3  42   0  43  11 106]\n",
      "이 냄새 는 시간 이 좀 지나 면 없 어 지 ㄹ 것 이 ㅂ니다 . \n",
      "은 그 시간 이 ㄴ 것 은 시간 을 하 ㄹ 수 도 모르 ㄴ다 . ( UNK ) 은 시간 을 하 ㄹ 수 도 모르 ㄴ다 . ( UNK ) 은 시간 을 하 ㄹ 수 도 모르 ㄴ다 . ( UNK ) 은 시간 을 하 ㄹ 수 도 모르 ㄴ다 . ( UNK ) 은 시간 \n"
     ]
    }
   ],
   "source": [
    "def evaluate(inp, enc_emb, dec_emb, BATCH_SIZE):\n",
    "\n",
    "  enc_inp = tf.nn.embedding_lookup(enc_emb, inp)\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  enc_output, enc_hidden = encoder(enc_inp, enc_hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "\n",
    "  dec_input = tf.expand_dims([1] * BATCH_SIZE, 1)\n",
    "\n",
    "  outputs = []\n",
    "  attention_weights = []\n",
    "  # 교사 강요(teacher forcing) - 다음 입력으로 타겟을 피딩(feeding)합니다.\n",
    "  for t in range(kr_length):\n",
    "    dec_input = tf.nn.embedding_lookup(dec_emb, dec_input)\n",
    "    # enc_output를 디코더에 전달합니다.\n",
    "    predictions, dec_hidden, attention_weight = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "    attention_weights.append(attention_weight)\n",
    "    #loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "    # 교사 강요(teacher forcing)를 사용합니다.\n",
    "    dec_input = tf.argmax(predictions, axis=-1)\n",
    "    outputs.append(dec_input)\n",
    "    dec_input = tf.expand_dims(dec_input, 1)\n",
    "    \n",
    "  outputs = np.stack(outputs, axis=1)\n",
    "  attention_weights = np.stack(attention_weights, axis=1)\n",
    "  return outputs, attention_weights\n",
    "      \n",
    "\n",
    "inp, targ, _, _ = batch_generator.make_batch(batch_size, en_length, kr_length)\n",
    "  \n",
    "outputs, attention_weights = evaluate(inp, eng_embedding, kor_embedding, batch_size)\n",
    "\n",
    "print(outputs.shape)\n",
    "print(attention_weights.shape)\n",
    "print(outputs[0])\n",
    "\n",
    "\n",
    "\n",
    "kor_outputs = token_to_kor(outputs)\n",
    "kor_targ = token_to_kor(targ)\n",
    "print(kor_targ[0])\n",
    "print(kor_outputs[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
